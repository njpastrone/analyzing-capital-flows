{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Case Study 1: Debug Analysis\n## Iceland vs Eurozone - Data Validation & Testing\n\n**Purpose:** Internal debugging and validation of analysis methodology"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug environment ready\n"
     ]
    }
   ],
   "source": [
    "# Setup with debugging capabilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Debug configuration\n",
    "DEBUG = True\n",
    "\n",
    "def debug_print(message):\n",
    "    if DEBUG:\n",
    "        print(f\"[DEBUG] {message}\")\n",
    "\n",
    "def validate_data(df, name):\n",
    "    print(f\"\\n=== {name} VALIDATION ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "    return df\n",
    "\n",
    "print(\"Debug environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading BOP data...\n",
      "\n",
      "=== BOP Data VALIDATION ===\n",
      "Shape: (15248, 8)\n",
      "Missing values: 2808\n",
      "Duplicates: 0\n",
      "\n",
      "Countries: ['Austria', 'Belgium', 'Finland', 'France', 'Germany', 'Iceland', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands, The', 'Portugal', 'Spain']\n",
      "Indicators: 6\n",
      "Time range: 1999-Q1 to 2025-Q1\n"
     ]
    }
   ],
   "source": [
    "# Load data with validation\n",
    "debug_print(\"Loading BOP data...\")\n",
    "case_one_raw = pd.read_csv(\"../../data/case_study_1_data_july_24_2025.csv\")\n",
    "case_one_raw = validate_data(case_one_raw, \"BOP Data\")\n",
    "\n",
    "print(f\"\\nCountries: {sorted(case_one_raw['COUNTRY'].unique())}\")\n",
    "print(f\"Indicators: {len(case_one_raw['INDICATOR'].unique())}\")\n",
    "print(f\"Time range: {case_one_raw['TIME_PERIOD'].min()} to {case_one_raw['TIME_PERIOD'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading GDP data...\n",
      "\n",
      "=== GDP Data VALIDATION ===\n",
      "Shape: (5571, 7)\n",
      "Missing values: 27\n",
      "Duplicates: 0\n",
      "\n",
      "Country overlap: 12 countries\n",
      "Common: ['Austria', 'Belgium', 'Finland', 'France', 'Germany', 'Iceland', 'Ireland', 'Italy', 'Luxembourg', 'Netherlands, The', 'Portugal', 'Spain']\n"
     ]
    }
   ],
   "source": [
    "# Load GDP data\n",
    "debug_print(\"Loading GDP data...\")\n",
    "gdp_raw = pd.read_csv(\"../../data/dataset_2025-07-24T18_28_31.898465539Z_DEFAULT_INTEGRATION_IMF.RES_WEO_6.0.0.csv\")\n",
    "gdp_raw = validate_data(gdp_raw, \"GDP Data\")\n",
    "\n",
    "# Check country overlap\n",
    "bop_countries = set(case_one_raw['COUNTRY'].unique())\n",
    "gdp_countries = set(gdp_raw['COUNTRY'].unique())\n",
    "common = bop_countries.intersection(gdp_countries)\n",
    "\n",
    "print(f\"\\nCountry overlap: {len(common)} countries\")\n",
    "print(f\"Common: {sorted(common)}\")\n",
    "if bop_countries - gdp_countries:\n",
    "    print(f\"BOP only: {bop_countries - gdp_countries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Processing BOP data...\n",
      "Cleaned BOP shape: (15248, 6)\n",
      "Indicators: 13\n",
      "Year range: 1999-2025\n"
     ]
    }
   ],
   "source": [
    "# Replicate main analysis pipeline\n",
    "debug_print(\"Processing BOP data...\")\n",
    "\n",
    "# Clean BOP data\n",
    "case_one_clean = case_one_raw.copy()\n",
    "case_one_clean['ENTRY_FIRST_WORD'] = case_one_clean['BOP_ACCOUNTING_ENTRY'].str.extract(r'^([^,]+)')\n",
    "case_one_clean['FULL_INDICATOR'] = case_one_clean['ENTRY_FIRST_WORD'] + ' - ' + case_one_clean['INDICATOR']\n",
    "\n",
    "# Drop columns and process time\n",
    "columns_to_drop = ['BOP_ACCOUNTING_ENTRY', 'INDICATOR', 'ENTRY_FIRST_WORD', 'FREQUENCY', 'SCALE']\n",
    "case_one_clean = case_one_clean.drop(columns=columns_to_drop)\n",
    "case_one_clean[['YEAR', 'QUARTER']] = case_one_clean['TIME_PERIOD'].str.split('-', expand=True)\n",
    "case_one_clean['YEAR'] = case_one_clean['YEAR'].astype(int)\n",
    "case_one_clean['QUARTER'] = case_one_clean['QUARTER'].str.extract(r'(\\d+)').astype(int)\n",
    "case_one_clean = case_one_clean.drop('TIME_PERIOD', axis=1)\n",
    "\n",
    "print(f\"Cleaned BOP shape: {case_one_clean.shape}\")\n",
    "print(f\"Indicators: {len(case_one_clean['FULL_INDICATOR'].unique())}\")\n",
    "print(f\"Year range: {case_one_clean['YEAR'].min()}-{case_one_clean['YEAR'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Pivoting and joining data...\n",
      "Merged data shape: (1186, 18)\n",
      "GDP column available: True\n"
     ]
    }
   ],
   "source": [
    "# Process and join data\n",
    "debug_print(\"Pivoting and joining data...\")\n",
    "\n",
    "# Pivot BOP data\n",
    "bop_pivoted = case_one_clean.pivot_table(\n",
    "    index=['COUNTRY', 'YEAR', 'QUARTER', 'UNIT'],\n",
    "    columns='FULL_INDICATOR',\n",
    "    values='OBS_VALUE',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Process GDP\n",
    "gdp_clean = gdp_raw[['COUNTRY', 'TIME_PERIOD', 'INDICATOR', 'OBS_VALUE']].copy()\n",
    "gdp_pivoted = gdp_clean.pivot_table(\n",
    "    index=['COUNTRY', 'TIME_PERIOD'],\n",
    "    columns='INDICATOR',\n",
    "    values='OBS_VALUE',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Join datasets\n",
    "merged_data = bop_pivoted.merge(\n",
    "    gdp_pivoted,\n",
    "    left_on=['COUNTRY', 'YEAR'],\n",
    "    right_on=['COUNTRY', 'TIME_PERIOD'],\n",
    "    how='left'\n",
    ").drop('TIME_PERIOD', axis=1, errors='ignore')\n",
    "\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(f\"GDP column available: {'Gross domestic product (GDP), Current prices, US dollar' in merged_data.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Normalizing to % of GDP...\n",
      "Normalized data shape: (1186, 18)\n",
      "BOP indicators normalized: 13\n",
      "GDP zero values: 0\n",
      "GDP null values: 0\n"
     ]
    }
   ],
   "source": [
    "# Normalize to % of GDP\n",
    "debug_print(\"Normalizing to % of GDP...\")\n",
    "\n",
    "metadata_cols = ['COUNTRY', 'YEAR', 'QUARTER', 'UNIT']\n",
    "gdp_col = 'Gross domestic product (GDP), Current prices, US dollar'\n",
    "bop_cols = [col for col in merged_data.columns if col not in metadata_cols + [gdp_col]]\n",
    "\n",
    "# Create normalized dataset\n",
    "normalized_data = merged_data[metadata_cols + [gdp_col]].copy()\n",
    "for col in bop_cols:\n",
    "    normalized_data[f\"{col}_PGDP\"] = (merged_data[col] * 4 / merged_data[gdp_col]) * 100\n",
    "\n",
    "normalized_data['UNIT'] = \"% of GDP (annualized)\"\n",
    "\n",
    "print(f\"Normalized data shape: {normalized_data.shape}\")\n",
    "print(f\"BOP indicators normalized: {len(bop_cols)}\")\n",
    "\n",
    "# Check for normalization issues\n",
    "gdp_zeros = (merged_data[gdp_col] == 0).sum()\n",
    "gdp_nulls = merged_data[gdp_col].isnull().sum()\n",
    "print(f\"GDP zero values: {gdp_zeros}\")\n",
    "print(f\"GDP null values: {gdp_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating country groups...\n",
      "Final data shape: (1093, 19)\n",
      "Group distribution:\n",
      "GROUP\n",
      "Eurozone    988\n",
      "Iceland     105\n",
      "Name: count, dtype: int64\n",
      "Iceland: ['Iceland']\n",
      "Eurozone: ['Austria', 'Belgium', 'Finland', 'France', 'Germany', 'Ireland', 'Italy', 'Netherlands, The', 'Portugal', 'Spain']\n",
      "\n",
      "Analysis indicators: 13\n"
     ]
    }
   ],
   "source": [
    "# Create country groups\n",
    "debug_print(\"Creating country groups...\")\n",
    "\n",
    "# Add groups\n",
    "normalized_data['GROUP'] = normalized_data['COUNTRY'].apply(\n",
    "    lambda x: 'Iceland' if x == 'Iceland' else 'Eurozone'\n",
    ")\n",
    "\n",
    "# Remove Luxembourg\n",
    "final_data = normalized_data[normalized_data['COUNTRY'] != 'Luxembourg'].copy()\n",
    "\n",
    "print(f\"Final data shape: {final_data.shape}\")\n",
    "print(f\"Group distribution:\")\n",
    "print(final_data['GROUP'].value_counts())\n",
    "\n",
    "# Show countries by group\n",
    "for group in ['Iceland', 'Eurozone']:\n",
    "    countries = sorted(final_data[final_data['GROUP'] == group]['COUNTRY'].unique())\n",
    "    print(f\"{group}: {countries}\")\n",
    "\n",
    "# Get analysis indicators\n",
    "analysis_indicators = [col for col in final_data.columns if col.endswith('_PGDP')]\n",
    "print(f\"\\nAnalysis indicators: {len(analysis_indicators)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Running statistical tests...\n",
      "\n",
      "TEST RESULTS SUMMARY:\n",
      "Indicators tested: 13\n",
      "Iceland higher volatility: 9/13 (69.2%)\n",
      "Statistically significant: 13/13 (100.0%)\n",
      "\n",
      "Top 5 by F-statistic:\n",
      "  Liabilities - Portfolio invest... F=  7.93 p= 0.0000\n",
      "  Net (net acquisition of financ... F=  3.81 p= 0.0000\n",
      "  Net (net acquisition of financ... F=  3.39 p= 0.0000\n",
      "  Assets - Other investment, Deb... F=  2.82 p= 0.0000\n",
      "  Liabilities - Other investment... F=  2.70 p= 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Perform statistical tests with validation\n",
    "debug_print(\"Running statistical tests...\")\n",
    "\n",
    "def test_volatility_with_validation(data, indicator):\n",
    "    \"\"\"Test volatility with comprehensive validation\"\"\"\n",
    "    iceland_data = data[data['GROUP'] == 'Iceland'][indicator].dropna()\n",
    "    eurozone_data = data[data['GROUP'] == 'Eurozone'][indicator].dropna()\n",
    "    \n",
    "    if len(iceland_data) < 2 or len(eurozone_data) < 2:\n",
    "        return {'error': 'Insufficient data'}\n",
    "    \n",
    "    # Basic statistics\n",
    "    iceland_stats = {\n",
    "        'n': len(iceland_data),\n",
    "        'mean': iceland_data.mean(),\n",
    "        'std': iceland_data.std(),\n",
    "        'var': iceland_data.var()\n",
    "    }\n",
    "    \n",
    "    eurozone_stats = {\n",
    "        'n': len(eurozone_data),\n",
    "        'mean': eurozone_data.mean(),\n",
    "        'std': eurozone_data.std(),\n",
    "        'var': eurozone_data.var()\n",
    "    }\n",
    "    \n",
    "    # F-test\n",
    "    f_stat = iceland_stats['var'] / eurozone_stats['var'] if eurozone_stats['var'] != 0 else np.inf\n",
    "    df1, df2 = iceland_stats['n'] - 1, eurozone_stats['n'] - 1\n",
    "    p_value = 2 * min(stats.f.cdf(f_stat, df1, df2), 1 - stats.f.cdf(f_stat, df1, df2))\n",
    "    \n",
    "    return {\n",
    "        'iceland_stats': iceland_stats,\n",
    "        'eurozone_stats': eurozone_stats,\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_value,\n",
    "        'iceland_more_volatile': iceland_stats['var'] > eurozone_stats['var']\n",
    "    }\n",
    "\n",
    "# Test all indicators\n",
    "test_results = []\n",
    "for indicator in analysis_indicators:\n",
    "    result = test_volatility_with_validation(final_data, indicator)\n",
    "    if 'error' not in result:\n",
    "        test_results.append({\n",
    "            'Indicator': indicator.replace('_PGDP', ''),\n",
    "            'F_Statistic': result['f_statistic'],\n",
    "            'P_Value': result['p_value'],\n",
    "            'Iceland_More_Volatile': result['iceland_more_volatile'],\n",
    "            'Significant': result['p_value'] < 0.05\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(test_results)\n",
    "\n",
    "print(f\"\\nTEST RESULTS SUMMARY:\")\n",
    "print(f\"Indicators tested: {len(results_df)}\")\n",
    "if len(results_df) > 0:\n",
    "    iceland_higher = results_df['Iceland_More_Volatile'].sum()\n",
    "    significant = results_df['Significant'].sum()\n",
    "    print(f\"Iceland higher volatility: {iceland_higher}/{len(results_df)} ({iceland_higher/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"Statistically significant: {significant}/{len(results_df)} ({significant/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop 5 by F-statistic:\")\n",
    "    top_5 = results_df.nlargest(5, 'F_Statistic')\n",
    "    for _, row in top_5.iterrows():\n",
    "        name = row['Indicator'][:30] + '...' if len(row['Indicator']) > 30 else row['Indicator']\n",
    "        print(f\"  {name:<33} F={row['F_Statistic']:6.2f} p={row['P_Value']:7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Performing comprehensive data quality assessment...\n",
      "\n",
      "COMPREHENSIVE DATA QUALITY ASSESSMENT:\n",
      "============================================================\n",
      "\n",
      "1. MISSING DATA ANALYSIS (All Indicators):\n",
      "--------------------------------------------------\n",
      "Assets - Direct investment, Total f... Total:   0.0% Iceland:   0.0% Eurozone:   0.0%\n",
      "Assets - Other investment, Debt ins... Total:   2.7% Iceland:   0.0% Eurozone:   2.9%\n",
      "Assets - Other investment, Debt ins... Total:   5.7% Iceland:   0.0% Eurozone:   6.3%\n",
      "Assets - Portfolio investment, Debt... Total:   0.0% Iceland:   0.0% Eurozone:   0.0%\n",
      "Assets - Portfolio investment, Equi... Total:   0.0% Iceland:   0.0% Eurozone:   0.0%\n",
      "Assets - Portfolio investment, Tota... Total:   0.0% Iceland:   0.0% Eurozone:   0.0%\n",
      "Liabilities - Direct investment, To... Total:   0.0% Iceland:   0.0% Eurozone:   0.0%\n",
      "Liabilities - Other investment, Deb... Total:   6.5% Iceland:   0.0% Eurozone:   7.2%\n",
      "Liabilities - Portfolio investment,... Total:   0.4% Iceland:   0.0% Eurozone:   0.4%\n",
      "Liabilities - Portfolio investment,... Total:   0.4% Iceland:   0.0% Eurozone:   0.4%\n",
      "Liabilities - Portfolio investment,... Total:   0.0% Iceland:   0.0% Eurozone:   0.0%\n",
      "Net (net acquisition of financial a... Total:   0.0% Iceland:   0.0% Eurozone:   0.0%\n",
      "Net (net acquisition of financial a... Total:   0.0% Iceland:   0.0% Eurozone:   0.0%\n",
      "\n",
      "✓ No indicators with excessive missing data (>20%)\n",
      "\n",
      "\n",
      "2. OUTLIER DETECTION (All Indicators - IQR Method):\n",
      "-------------------------------------------------------\n",
      "Assets - Direct investment, Total f... Outliers: 190 (17.4%) Extreme: 120\n",
      "Assets - Other investment, Debt ins... Outliers: 138 (13.0%) Extreme: 58\n",
      "Assets - Other investment, Debt ins... Outliers: 113 (11.0%) Extreme: 42\n",
      "Assets - Portfolio investment, Debt... Outliers: 113 (10.3%) Extreme: 64\n",
      "Assets - Portfolio investment, Equi... Outliers: 125 (11.4%) Extreme: 70\n",
      "Assets - Portfolio investment, Tota... Outliers: 109 (10.0%) Extreme: 69\n",
      "Liabilities - Direct investment, To... Outliers: 199 (18.2%) Extreme: 123\n",
      "Liabilities - Other investment, Deb... Outliers: 129 (12.6%) Extreme: 50\n",
      "Liabilities - Portfolio investment,... Outliers:  70 ( 6.4%) Extreme: 33\n",
      "Liabilities - Portfolio investment,... Outliers: 145 (13.3%) Extreme: 102\n",
      "Liabilities - Portfolio investment,... Outliers: 100 ( 9.1%) Extreme: 68\n",
      "Net (net acquisition of financial a... Outliers: 149 (13.6%) Extreme: 72\n",
      "Net (net acquisition of financial a... Outliers:  83 ( 7.6%) Extreme: 34\n",
      "\n",
      "⚠️  HIGH OUTLIER RATES (>15%): 2 indicators\n",
      "   - Assets - Direct investment, Total financial assets: 17.4%\n",
      "   - Liabilities - Direct investment, Total financial a: 18.2%\n",
      "\n",
      "\n",
      "3. DATA DISTRIBUTION ANALYSIS (All Indicators):\n",
      "--------------------------------------------------\n",
      "Assets - Direct investment, To... CV: Ice= 742.2% Euro= 294.7% Ratio= 2.52\n",
      "Assets - Other investment, Deb... CV: Ice= 436.5% Euro= 530.2% Ratio= 0.82\n",
      "Assets - Other investment, Deb... CV: Ice= 280.4% Euro= 781.2% Ratio= 0.36\n",
      "Assets - Portfolio investment,... CV: Ice=1067.6% Euro= 355.8% Ratio= 3.00\n",
      "Assets - Portfolio investment,... CV: Ice= 274.0% Euro= 254.1% Ratio= 1.08\n",
      "Assets - Portfolio investment,... CV: Ice= 381.4% Euro= 275.2% Ratio= 1.39\n",
      "Liabilities - Direct investmen... CV: Ice= 398.9% Euro= 340.3% Ratio= 1.17\n",
      "Liabilities - Other investment... CV: Ice= 361.9% Euro=1053.1% Ratio= 0.34\n",
      "Liabilities - Portfolio invest... CV: Ice= 296.8% Euro= 287.9% Ratio= 1.03\n",
      "Liabilities - Portfolio invest... CV: Ice=2234.6% Euro= 384.5% Ratio= 5.81\n",
      "Liabilities - Portfolio invest... CV: Ice= 300.9% Euro= 268.7% Ratio= 1.12\n",
      "Net (net acquisition of financ... CV: Ice=2165.0% Euro=1394.0% Ratio= 1.55\n",
      "Net (net acquisition of financ... CV: Ice= 484.6% Euro=5522.6% Ratio= 0.09\n",
      "\n",
      "\n",
      "4. SAMPLE SIZE ADEQUACY BY INDICATOR:\n",
      "---------------------------------------------\n",
      "Assets - Direct investment, To... Iceland: 105✓ Eurozone: 988✓\n",
      "Assets - Other investment, Deb... Iceland: 105✓ Eurozone: 959✓\n",
      "Assets - Other investment, Deb... Iceland: 105✓ Eurozone: 926✓\n",
      "Assets - Portfolio investment,... Iceland: 105✓ Eurozone: 988✓\n",
      "Assets - Portfolio investment,... Iceland: 105✓ Eurozone: 988✓\n",
      "Assets - Portfolio investment,... Iceland: 105✓ Eurozone: 988✓\n",
      "Liabilities - Direct investmen... Iceland: 105✓ Eurozone: 988✓\n",
      "Liabilities - Other investment... Iceland: 105✓ Eurozone: 917✓\n",
      "Liabilities - Portfolio invest... Iceland: 105✓ Eurozone: 984✓\n",
      "Liabilities - Portfolio invest... Iceland: 105✓ Eurozone: 984✓\n",
      "Liabilities - Portfolio invest... Iceland: 105✓ Eurozone: 988✓\n",
      "Net (net acquisition of financ... Iceland: 105✓ Eurozone: 988✓\n",
      "Net (net acquisition of financ... Iceland: 105✓ Eurozone: 988✓\n",
      "\n",
      "\n",
      "5. TIME COVERAGE BY GROUP:\n",
      "------------------------------\n",
      "Iceland:\n",
      "   Time span: 1999-2025 (27 years)\n",
      "   Unique years: 27/27 (100.0% coverage)\n",
      "   Avg quarters/year: 3.9\n",
      "   Total observations: 105\n",
      "\n",
      "Eurozone:\n",
      "   Time span: 1999-2025 (27 years)\n",
      "   Unique years: 27/27 (100.0% coverage)\n",
      "   Avg quarters/year: 3.9\n",
      "   Countries: 10, Avg obs/country: 98.8\n",
      "   Total observations: 988\n",
      "\n",
      "\n",
      "6. EXTREME VALUES ANALYSIS:\n",
      "-----------------------------------\n",
      "Found extreme values in 13 indicators:\n",
      "  Assets - Direct investment, To... Min:   -136.6 Max:    115.6 VeryExtreme: 0\n",
      "  Assets - Other investment, Deb... Min:   -147.1 Max:    158.7 VeryExtreme: 0\n",
      "  Assets - Other investment, Deb... Min:   -141.7 Max:    161.7 VeryExtreme: 0\n",
      "  Assets - Portfolio investment,... Min:   -200.1 Max:    136.9 VeryExtreme: 0\n",
      "  Assets - Portfolio investment,... Min:    -56.7 Max:     70.9 VeryExtreme: 0\n",
      "  Assets - Portfolio investment,... Min:   -175.4 Max:    162.5 VeryExtreme: 0\n",
      "  Liabilities - Direct investmen... Min:   -121.7 Max:    219.4 VeryExtreme: 0\n",
      "  Liabilities - Other investment... Min:   -139.7 Max:    200.4 VeryExtreme: 0\n",
      "  Liabilities - Portfolio invest... Min:   -157.9 Max:    128.0 VeryExtreme: 0\n",
      "  Liabilities - Portfolio invest... Min:   -116.8 Max:    157.9 VeryExtreme: 0\n",
      "  Liabilities - Portfolio invest... Min:   -161.6 Max:    183.7 VeryExtreme: 0\n",
      "  Net (net acquisition of financ... Min:   -133.9 Max:     75.7 VeryExtreme: 0\n",
      "  Net (net acquisition of financ... Min:   -230.6 Max:    107.8 VeryExtreme: 0\n",
      "\n",
      "\n",
      "7. EXPORTING COMPREHENSIVE QUALITY ASSESSMENT:\n",
      "--------------------------------------------------\n",
      "✓ Comprehensive quality assessment saved: comprehensive_data_quality_assessment.csv\n",
      "\n",
      "\n",
      "QUALITY ASSESSMENT SUMMARY:\n",
      "========================================\n",
      "⚠️  POTENTIAL ISSUES IDENTIFIED:\n",
      "   - High outlier rates: 2 indicators (>15%)\n",
      "   - Extreme values: 13 indicators with very high/low values\n",
      "\n",
      "📊 RECOMMENDATION: Review the exported CSV for detailed quality metrics\n",
      "🔍 NEXT STEP: Consider excluding problematic indicators or using robust statistics\n",
      "\n",
      "📈 OVERALL ASSESSMENT: Data quality is acceptable for research purposes\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data quality assessment for ALL indicators\n",
    "debug_print(\"Performing comprehensive data quality assessment...\")\n",
    "\n",
    "print(\"\\nCOMPREHENSIVE DATA QUALITY ASSESSMENT:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Missing data analysis for ALL indicators\n",
    "print(\"\\n1. MISSING DATA ANALYSIS (All Indicators):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "missing_data_summary = []\n",
    "for indicator in analysis_indicators:\n",
    "    clean_name = indicator.replace('_PGDP', '')\n",
    "    \n",
    "    # Overall missing data\n",
    "    total_missing = final_data[indicator].isnull().sum()\n",
    "    total_obs = len(final_data)\n",
    "    missing_pct = (total_missing / total_obs) * 100\n",
    "    \n",
    "    # Missing by group\n",
    "    iceland_missing = final_data[final_data['GROUP'] == 'Iceland'][indicator].isnull().sum()\n",
    "    iceland_total = len(final_data[final_data['GROUP'] == 'Iceland'])\n",
    "    iceland_missing_pct = (iceland_missing / iceland_total) * 100 if iceland_total > 0 else 0\n",
    "    \n",
    "    eurozone_missing = final_data[final_data['GROUP'] == 'Eurozone'][indicator].isnull().sum()\n",
    "    eurozone_total = len(final_data[final_data['GROUP'] == 'Eurozone'])\n",
    "    eurozone_missing_pct = (eurozone_missing / eurozone_total) * 100 if eurozone_total > 0 else 0\n",
    "    \n",
    "    missing_data_summary.append({\n",
    "        'Indicator': clean_name,\n",
    "        'Total_Missing_Pct': missing_pct,\n",
    "        'Iceland_Missing_Pct': iceland_missing_pct,\n",
    "        'Eurozone_Missing_Pct': eurozone_missing_pct,\n",
    "        'Total_Missing_Count': total_missing\n",
    "    })\n",
    "    \n",
    "    # Display with truncated name for readability\n",
    "    display_name = clean_name[:35] + '...' if len(clean_name) > 35 else clean_name\n",
    "    print(f\"{display_name:<38} Total: {missing_pct:5.1f}% Iceland: {iceland_missing_pct:5.1f}% Eurozone: {eurozone_missing_pct:5.1f}%\")\n",
    "\n",
    "# Identify problematic indicators (>20% missing)\n",
    "high_missing = [item for item in missing_data_summary if item['Total_Missing_Pct'] > 20]\n",
    "if high_missing:\n",
    "    print(f\"\\n⚠️  HIGH MISSING DATA (>20%): {len(high_missing)} indicators\")\n",
    "    for item in high_missing:\n",
    "        print(f\"   - {item['Indicator'][:50]}: {item['Total_Missing_Pct']:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n✓ No indicators with excessive missing data (>20%)\")\n",
    "\n",
    "# 2. Outlier detection for ALL indicators\n",
    "print(\"\\n\\n2. OUTLIER DETECTION (All Indicators - IQR Method):\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "outlier_summary = []\n",
    "for indicator in analysis_indicators:\n",
    "    clean_name = indicator.replace('_PGDP', '')\n",
    "    data_values = final_data[indicator].dropna()\n",
    "    \n",
    "    if len(data_values) > 0:\n",
    "        # IQR method\n",
    "        Q1 = data_values.quantile(0.25)\n",
    "        Q3 = data_values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = ((data_values < lower_bound) | (data_values > upper_bound)).sum()\n",
    "        outlier_pct = (outliers / len(data_values)) * 100\n",
    "        \n",
    "        # Extreme outliers (beyond 3*IQR)\n",
    "        extreme_lower = Q1 - 3 * IQR\n",
    "        extreme_upper = Q3 + 3 * IQR\n",
    "        extreme_outliers = ((data_values < extreme_lower) | (data_values > extreme_upper)).sum()\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Indicator': clean_name,\n",
    "            'Total_Observations': len(data_values),\n",
    "            'Outliers_Count': outliers,\n",
    "            'Outliers_Pct': outlier_pct,\n",
    "            'Extreme_Outliers': extreme_outliers,\n",
    "            'Min_Value': data_values.min(),\n",
    "            'Max_Value': data_values.max(),\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3\n",
    "        })\n",
    "        \n",
    "        display_name = clean_name[:35] + '...' if len(clean_name) > 35 else clean_name\n",
    "        print(f\"{display_name:<38} Outliers: {outliers:3d} ({outlier_pct:4.1f}%) Extreme: {extreme_outliers:2d}\")\n",
    "\n",
    "# Identify indicators with excessive outliers\n",
    "high_outliers = [item for item in outlier_summary if item['Outliers_Pct'] > 15]\n",
    "if high_outliers:\n",
    "    print(f\"\\n⚠️  HIGH OUTLIER RATES (>15%): {len(high_outliers)} indicators\")\n",
    "    for item in high_outliers:\n",
    "        print(f\"   - {item['Indicator'][:50]}: {item['Outliers_Pct']:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n✓ No indicators with excessive outliers (>15%)\")\n",
    "\n",
    "# 3. Data distribution analysis for ALL indicators\n",
    "print(\"\\n\\n3. DATA DISTRIBUTION ANALYSIS (All Indicators):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "distribution_summary = []\n",
    "for indicator in analysis_indicators:\n",
    "    clean_name = indicator.replace('_PGDP', '')\n",
    "    \n",
    "    # Iceland data\n",
    "    iceland_data = final_data[final_data['GROUP'] == 'Iceland'][indicator].dropna()\n",
    "    # Eurozone data  \n",
    "    eurozone_data = final_data[final_data['GROUP'] == 'Eurozone'][indicator].dropna()\n",
    "    \n",
    "    if len(iceland_data) > 0 and len(eurozone_data) > 0:\n",
    "        # Calculate key statistics\n",
    "        iceland_stats = {\n",
    "            'mean': iceland_data.mean(),\n",
    "            'std': iceland_data.std(),\n",
    "            'skew': stats.skew(iceland_data) if len(iceland_data) > 2 else np.nan,\n",
    "            'kurtosis': stats.kurtosis(iceland_data) if len(iceland_data) > 2 else np.nan\n",
    "        }\n",
    "        \n",
    "        eurozone_stats = {\n",
    "            'mean': eurozone_data.mean(),\n",
    "            'std': eurozone_data.std(), \n",
    "            'skew': stats.skew(eurozone_data) if len(eurozone_data) > 2 else np.nan,\n",
    "            'kurtosis': stats.kurtosis(eurozone_data) if len(eurozone_data) > 2 else np.nan\n",
    "        }\n",
    "        \n",
    "        # Coefficient of variation\n",
    "        iceland_cv = (iceland_stats['std'] / abs(iceland_stats['mean'])) * 100 if iceland_stats['mean'] != 0 else np.inf\n",
    "        eurozone_cv = (eurozone_stats['std'] / abs(eurozone_stats['mean'])) * 100 if eurozone_stats['mean'] != 0 else np.inf\n",
    "        \n",
    "        distribution_summary.append({\n",
    "            'Indicator': clean_name,\n",
    "            'Iceland_CV': iceland_cv,\n",
    "            'Eurozone_CV': eurozone_cv,\n",
    "            'CV_Ratio': iceland_cv / eurozone_cv if eurozone_cv != 0 else np.inf,\n",
    "            'Iceland_Skew': iceland_stats['skew'],\n",
    "            'Eurozone_Skew': eurozone_stats['skew'],\n",
    "            'Iceland_Kurtosis': iceland_stats['kurtosis'],\n",
    "            'Eurozone_Kurtosis': eurozone_stats['kurtosis']\n",
    "        })\n",
    "        \n",
    "        display_name = clean_name[:30] + '...' if len(clean_name) > 30 else clean_name\n",
    "        print(f\"{display_name:<33} CV: Ice={iceland_cv:6.1f}% Euro={eurozone_cv:6.1f}% Ratio={iceland_cv/eurozone_cv if eurozone_cv != 0 else np.inf:5.2f}\")\n",
    "\n",
    "# 4. Sample size adequacy by indicator\n",
    "print(\"\\n\\n4. SAMPLE SIZE ADEQUACY BY INDICATOR:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "sample_size_issues = []\n",
    "for indicator in analysis_indicators:\n",
    "    clean_name = indicator.replace('_PGDP', '')\n",
    "    \n",
    "    iceland_n = final_data[final_data['GROUP'] == 'Iceland'][indicator].notna().sum()\n",
    "    eurozone_n = final_data[final_data['GROUP'] == 'Eurozone'][indicator].notna().sum()\n",
    "    \n",
    "    # Check adequacy (rule of thumb: >30 for CLT, >10 for basic stats)\n",
    "    iceland_adequate = \"✓\" if iceland_n >= 30 else \"⚠️\" if iceland_n >= 10 else \"❌\"\n",
    "    eurozone_adequate = \"✓\" if eurozone_n >= 30 else \"⚠️\" if eurozone_n >= 10 else \"❌\"\n",
    "    \n",
    "    if iceland_n < 30 or eurozone_n < 30:\n",
    "        sample_size_issues.append({\n",
    "            'Indicator': clean_name,\n",
    "            'Iceland_N': iceland_n,\n",
    "            'Eurozone_N': eurozone_n\n",
    "        })\n",
    "    \n",
    "    # Show sample sizes for all indicators\n",
    "    display_name = clean_name[:30] + '...' if len(clean_name) > 30 else clean_name\n",
    "    print(f\"{display_name:<33} Iceland: {iceland_n:3d}{iceland_adequate} Eurozone: {eurozone_n:3d}{eurozone_adequate}\")\n",
    "\n",
    "if sample_size_issues:\n",
    "    print(f\"\\n⚠️  SAMPLE SIZE CONCERNS: {len(sample_size_issues)} indicators with n<30\")\n",
    "    for item in sample_size_issues:\n",
    "        print(f\"   - {item['Indicator'][:45]}: Iceland={item['Iceland_N']}, Eurozone={item['Eurozone_N']}\")\n",
    "\n",
    "# 5. Time coverage consistency \n",
    "print(\"\\n\\n5. TIME COVERAGE BY GROUP:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for group in ['Iceland', 'Eurozone']:\n",
    "    group_data = final_data[final_data['GROUP'] == group]\n",
    "    min_year = group_data['YEAR'].min()\n",
    "    max_year = group_data['YEAR'].max()\n",
    "    year_span = max_year - min_year + 1\n",
    "    \n",
    "    # Check for gaps in years\n",
    "    unique_years = group_data['YEAR'].nunique()\n",
    "    quarters_per_year = group_data.groupby('YEAR')['QUARTER'].nunique().mean()\n",
    "    \n",
    "    if group == 'Iceland':\n",
    "        total_observations = len(group_data)\n",
    "        countries = 1\n",
    "    else:\n",
    "        countries = group_data['COUNTRY'].nunique() \n",
    "        total_observations = len(group_data)\n",
    "        obs_per_country = total_observations / countries\n",
    "        \n",
    "    print(f\"{group}:\")\n",
    "    print(f\"   Time span: {min_year}-{max_year} ({year_span} years)\")\n",
    "    print(f\"   Unique years: {unique_years}/{year_span} ({unique_years/year_span*100:.1f}% coverage)\")\n",
    "    print(f\"   Avg quarters/year: {quarters_per_year:.1f}\")\n",
    "    if group == 'Eurozone':\n",
    "        print(f\"   Countries: {countries}, Avg obs/country: {obs_per_country:.1f}\")\n",
    "    print(f\"   Total observations: {total_observations}\\n\")\n",
    "\n",
    "# 6. Extreme value analysis\n",
    "print(\"\\n6. EXTREME VALUES ANALYSIS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "extreme_values = []\n",
    "for indicator in analysis_indicators:\n",
    "    clean_name = indicator.replace('_PGDP', '')\n",
    "    data_values = final_data[indicator].dropna()\n",
    "    \n",
    "    if len(data_values) > 0:\n",
    "        # Find extreme values (beyond 99th percentile or below 1st percentile)\n",
    "        p1 = data_values.quantile(0.01)\n",
    "        p99 = data_values.quantile(0.99)\n",
    "        \n",
    "        extreme_low = (data_values < p1).sum()\n",
    "        extreme_high = (data_values > p99).sum()\n",
    "        \n",
    "        # Check for values that might be data errors (e.g., >1000% of GDP)\n",
    "        very_extreme = (abs(data_values) > 1000).sum()\n",
    "        \n",
    "        if extreme_low > 0 or extreme_high > 0 or very_extreme > 0:\n",
    "            extreme_values.append({\n",
    "                'Indicator': clean_name,\n",
    "                'Extreme_Low': extreme_low,\n",
    "                'Extreme_High': extreme_high,\n",
    "                'Very_Extreme': very_extreme,\n",
    "                'Min': data_values.min(),\n",
    "                'Max': data_values.max(),\n",
    "                'P1': p1,\n",
    "                'P99': p99\n",
    "            })\n",
    "\n",
    "if extreme_values:\n",
    "    print(f\"Found extreme values in {len(extreme_values)} indicators:\")\n",
    "    for item in extreme_values:\n",
    "        display_name = item['Indicator'][:30] + '...' if len(item['Indicator']) > 30 else item['Indicator']\n",
    "        print(f\"  {display_name:<33} Min: {item['Min']:8.1f} Max: {item['Max']:8.1f} VeryExtreme: {item['Very_Extreme']}\")\n",
    "else:\n",
    "    print(\"✓ No extreme values found (>99th percentile or <1st percentile)\")\n",
    "\n",
    "# 7. Export comprehensive quality assessment\n",
    "print(\"\\n\\n7. EXPORTING COMPREHENSIVE QUALITY ASSESSMENT:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create comprehensive quality DataFrame\n",
    "quality_assessment = pd.DataFrame([\n",
    "    {\n",
    "        'Indicator': item['Indicator'],\n",
    "        'Missing_Data_Pct': missing_data_summary[i]['Total_Missing_Pct'],\n",
    "        'Iceland_Missing_Pct': missing_data_summary[i]['Iceland_Missing_Pct'],\n",
    "        'Eurozone_Missing_Pct': missing_data_summary[i]['Eurozone_Missing_Pct'],\n",
    "        'Outliers_Pct': outlier_summary[i]['Outliers_Pct'] if i < len(outlier_summary) else np.nan,\n",
    "        'Extreme_Outliers': outlier_summary[i]['Extreme_Outliers'] if i < len(outlier_summary) else np.nan,\n",
    "        'Iceland_CV': distribution_summary[i]['Iceland_CV'] if i < len(distribution_summary) else np.nan,\n",
    "        'Eurozone_CV': distribution_summary[i]['Eurozone_CV'] if i < len(distribution_summary) else np.nan,\n",
    "        'CV_Ratio': distribution_summary[i]['CV_Ratio'] if i < len(distribution_summary) else np.nan,\n",
    "        'Iceland_Sample_Size': final_data[final_data['GROUP'] == 'Iceland'][analysis_indicators[i]].notna().sum(),\n",
    "        'Eurozone_Sample_Size': final_data[final_data['GROUP'] == 'Eurozone'][analysis_indicators[i]].notna().sum(),\n",
    "        'Min_Value': outlier_summary[i]['Min_Value'] if i < len(outlier_summary) else np.nan,\n",
    "        'Max_Value': outlier_summary[i]['Max_Value'] if i < len(outlier_summary) else np.nan\n",
    "    }\n",
    "    for i, item in enumerate(missing_data_summary)\n",
    "])\n",
    "\n",
    "quality_assessment.to_csv('comprehensive_data_quality_assessment.csv', index=False)\n",
    "print(f\"✓ Comprehensive quality assessment saved: comprehensive_data_quality_assessment.csv\")\n",
    "\n",
    "# Summary of quality issues\n",
    "print(\"\\n\\nQUALITY ASSESSMENT SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "total_indicators = len(analysis_indicators)\n",
    "issues_found = []\n",
    "\n",
    "if high_missing:\n",
    "    issues_found.append(f\"High missing data: {len(high_missing)} indicators (>20%)\")\n",
    "if high_outliers:\n",
    "    issues_found.append(f\"High outlier rates: {len(high_outliers)} indicators (>15%)\")\n",
    "if sample_size_issues:\n",
    "    issues_found.append(f\"Sample size concerns: {len(sample_size_issues)} indicators (n<30)\")\n",
    "if extreme_values:\n",
    "    issues_found.append(f\"Extreme values: {len(extreme_values)} indicators with very high/low values\")\n",
    "\n",
    "if issues_found:\n",
    "    print(f\"⚠️  POTENTIAL ISSUES IDENTIFIED:\")\n",
    "    for issue in issues_found:\n",
    "        print(f\"   - {issue}\")\n",
    "    \n",
    "    print(f\"\\n📊 RECOMMENDATION: Review the exported CSV for detailed quality metrics\")\n",
    "    print(f\"🔍 NEXT STEP: Consider excluding problematic indicators or using robust statistics\")\n",
    "else:\n",
    "    print(f\"✅ EXCELLENT: No major data quality issues detected across {total_indicators} indicators\")\n",
    "    print(f\"✅ DATA READY: All indicators suitable for statistical analysis\")\n",
    "\n",
    "print(f\"\\n📈 OVERALL ASSESSMENT: Data quality is {'acceptable' if len(issues_found) <= 2 else 'concerning'} for research purposes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generating final summary...\n",
      "\n",
      "DEBUG ANALYSIS SUMMARY:\n",
      "========================================\n",
      "\n",
      "✓ Data loaded and validated\n",
      "✓ Processing pipeline replicated\n",
      "✓ 13 indicators normalized\n",
      "✓ Statistical tests completed\n",
      "✓ Data quality assessed\n",
      "\n",
      "KEY FINDINGS:\n",
      "  • 69.2% of indicators show Iceland has higher volatility\n",
      "  • 100.0% of tests are statistically significant\n",
      "  • CONCLUSION: Strong support for Hypothesis 1\n",
      "\n",
      "✓ Files exported: debug_final_dataset.csv, debug_test_results.csv\n",
      "\n",
      "🔍 Debug analysis complete - methodology validated\n"
     ]
    }
   ],
   "source": [
    "# Final summary and export\n",
    "debug_print(\"Generating final summary...\")\n",
    "\n",
    "print(\"\\nDEBUG ANALYSIS SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\n✓ Data loaded and validated\")\n",
    "print(f\"✓ Processing pipeline replicated\")\n",
    "print(f\"✓ {len(analysis_indicators)} indicators normalized\")\n",
    "print(f\"✓ Statistical tests completed\")\n",
    "print(f\"✓ Data quality assessed\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    iceland_higher_pct = results_df['Iceland_More_Volatile'].mean() * 100\n",
    "    significant_pct = results_df['Significant'].mean() * 100\n",
    "    \n",
    "    print(f\"\\nKEY FINDINGS:\")\n",
    "    print(f\"  • {iceland_higher_pct:.1f}% of indicators show Iceland has higher volatility\")\n",
    "    print(f\"  • {significant_pct:.1f}% of tests are statistically significant\")\n",
    "    \n",
    "    if iceland_higher_pct >= 60 and significant_pct >= 50:\n",
    "        print(f\"  • CONCLUSION: Strong support for Hypothesis 1\")\n",
    "    else:\n",
    "        print(f\"  • CONCLUSION: Mixed evidence for Hypothesis 1\")\n",
    "\n",
    "# Export results\n",
    "final_data.to_csv('debug_final_dataset.csv', index=False)\n",
    "if len(results_df) > 0:\n",
    "    results_df.to_csv('debug_test_results.csv', index=False)\n",
    "\n",
    "print(f\"\\n✓ Files exported: debug_final_dataset.csv, debug_test_results.csv\")\n",
    "print(f\"\\n🔍 Debug analysis complete - methodology validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Debug Analysis Complete\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "✅ **Data loading validation**  \n",
    "✅ **Processing pipeline verification**  \n",
    "✅ **Statistical testing with checks**  \n",
    "✅ **Data quality assessment**  \n",
    "✅ **Results export for review**  \n",
    "\n",
    "Use this to validate the main analysis and check for any methodology issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}